import time
import requests
import concurrent.futures
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


# --- CLASS CHA (BASE) ---
class BaseScraper:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        self.session.headers.update(self.headers)

    def _setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        prefs = {"profile.managed_default_content_settings.images": 2}
        chrome_options.add_experimental_option("prefs", prefs)

        try:
            return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        except Exception:
            return webdriver.Chrome(options=chrome_options)

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        """M·∫∑c ƒë·ªãnh: D√πng Scroll (Cho Viglacera Tiles)"""
        driver = None
        product_links = []
        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()

            if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p: {url}")
            driver.get(url)
            time.sleep(3)

            if progress_callback: progress_callback("üîÑ ƒêang cu·ªôn trang (Lazy Load)...")
            last_height = driver.execute_script("return document.body.scrollHeight")

            while True:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            items = soup.select(item_selector)

            if progress_callback: progress_callback(f"‚úÖ T√¨m th·∫•y {len(items)} th·∫ª s·∫£n ph·∫©m. ƒêang tr√≠ch xu·∫•t link...")
            domain = "/".join(url.split("/")[:3])

            for item in items:
                tag = item.select_one(link_selector) if link_selector else item
                if tag and tag.get('href'):
                    href = tag.get('href')
                    if not href.startswith('http'): href = domain + href
                    product_links.append(href)
        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()
        return list(set(product_links))

    def parse_detail(self, soup, url):
        raise NotImplementedError

    def _fetch_single_product(self, link):
        try:
            response = self.session.get(link, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                return self.parse_detail(soup, link)
        except Exception as e:
            print(f"L·ªói link {link}: {e}")
        return None

    def scrape_details_list(self, links, progress_bar=None, status_text=None):
        data = []
        total = len(links)
        MAX_WORKERS = 10
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self._fetch_single_product, link): link for link in links}
            completed = 0
            for future in concurrent.futures.as_completed(future_to_url):
                result = future.result()
                if result: data.append(result)
                completed += 1
                if progress_bar: progress_bar.progress(completed / total)
                if status_text: status_text.text(f"ƒê√£ t·∫£i xong: {completed}/{total} s·∫£n ph·∫©m")
        return data


# --- CLASS 1: Viglacera Tiles ---
class ViglaceraTilesScraper(BaseScraper):
    def parse_detail(self, soup, url):
        code_tag = soup.select_one('.title-main h2 strong')
        product_code = code_tag.text.strip() if code_tag else "N/A"
        breadcrumb = soup.select_one('.breadcrumb li:last-child a')
        collection = breadcrumb.text.strip() if breadcrumb else "N/A"
        images = []
        for img in soup.select('.detail-pic img'):
            src = img.get('src')
            if src:
                if not src.startswith('http'): src = "https://viglaceratiles.vn" + src
                images.append(src)
        dynamic_specs = {}
        for item in soup.select('.des-item'):
            key_tag = item.select_one('span')
            val_tag = item.select_one('h3')
            if key_tag and val_tag: dynamic_specs[key_tag.text.strip().title()] = val_tag.text.strip()
        final_data = {
            'URL': url, 'M√£ S·∫£n Ph·∫©m': product_code, 'B·ªô S∆∞u T·∫≠p': collection,
            '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A", 'Danh S√°ch ·∫¢nh': images
        }
        final_data.update(dynamic_specs)
        return final_data


# --- CLASS 2: Viglacera AAC ---
class ViglaceraAACScraper(BaseScraper):
    def parse_detail(self, soup, url):
        name_tag = soup.find('h1', itemprop='name')
        product_name = name_tag.text.strip() if name_tag else "N/A"
        brand_tag = soup.select_one('.pro-brand a')
        brand = brand_tag.text.strip() if brand_tag else "N/A"
        type_tag = soup.select_one('.pro-type a')
        product_type = type_tag.text.strip() if type_tag else "N/A"
        images = []
        main_img = soup.select_one('#ProductPhoto img')
        if main_img and main_img.get('src'):
            src = main_img.get('src')
            if src.startswith('//'): src = 'https:' + src
            images.append(src)
        for img in soup.select('#sliderproduct img'):
            src = img.get('src')
            if src:
                if src.startswith('//'): src = 'https:' + src
                if src not in images: images.append(src)
        specs = {}
        table = soup.find('table')
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all(['td', 'th'])
                row_data = [c.text.strip() for c in cols if c.text.strip()]
                if not row_data: continue
                if any(x in row_data[0].lower() for x in ['ch·ªâ ti√™u', 'th√¥ng s·ªë', 'ƒë∆°n v·ªã']): continue
                if len(row_data) >= 2:
                    key = row_data[0]
                    value = row_data[-1]
                    if len(row_data) > 2: key = f"{key} ({' '.join(row_data[1:-1])})"
                    specs[key] = value
        info_dict = {}
        headers = soup.find_all('h2')
        target_ul = None
        for h2 in headers:
            if any(x in h2.text.upper() for x in ["TH√îNG TIN", "T√çNH NƒÇNG"]):
                sibling = h2.find_next_sibling(['ul', 'div'])
                if sibling:
                    target_ul = sibling.find('ul') if sibling.name == 'div' else sibling
                    if target_ul: break
        if not target_ul:
            content_div = soup.find('div', class_='pro-tabcontent')
            if content_div: target_ul = content_div.find('ul')
        if target_ul:
            for i, li in enumerate(target_ul.find_all('li')):
                text = li.text.strip()
                if ':' in text:
                    k, v = text.split(':', 1)
                    info_dict[k.strip()] = v.strip()
                else:
                    info_dict[f"Th√¥ng tin {i + 1}"] = text
        final_data = {
            'URL': url, 'T√™n S·∫£n Ph·∫©m': product_name, 'Th∆∞∆°ng Hi·ªáu': brand,
            'Lo·∫°i S·∫£n Ph·∫©m': product_type, '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A", 'Danh S√°ch ·∫¢nh': images
        }
        final_data.update(info_dict)
        final_data.update(specs)
        return final_data


# --- CLASS 3: VTHM Group (Logic Data-Driven) ---
class VthmGroupScraper(BaseScraper):

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        driver = None
        product_links = set()  # S·ª≠ d·ª•ng SET ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i b·ªè link tr√πng

        # Selector ch·ªâ d√πng ƒë·ªÉ T√åM n√∫t, kh√¥ng d√πng ƒë·ªÉ check disabled n·ªØa
        NEXT_BUTTON_SELECTOR = "nav.pagination button.btn-next"

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()

            if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p: {url}")
            driver.get(url)
            time.sleep(5)

            page_count = 1
            page_des = 20
            last_first_link = ""  # Bi·∫øn ƒë·ªÉ ki·ªÉm tra trang ƒë√£ load xong ch∆∞a

            while True:
                # --- B∆Ø·ªöC 1: L·∫§Y D·ªÆ LI·ªÜU ---
                # Ch·ªù t·ªëi ƒëa 10s cho ƒë·∫øn khi link s·∫£n ph·∫©m ƒë·∫ßu ti√™n thay ƒë·ªïi so v·ªõi trang tr∆∞·ªõc
                retries = 0
                items = []
                while retries < 10:
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    items = soup.select(item_selector)

                    if not items: break
                    current_first_link = items[0].get('href')

                    # N·∫øu l√† trang 1 HO·∫∂C link ƒë·∫ßu ti√™n ƒë√£ kh√°c trang tr∆∞·ªõc -> OK, trang m·ªõi ƒë√£ load
                    if page_count == 1 or current_first_link != last_first_link:
                        last_first_link = current_first_link
                        break

                    time.sleep(1)
                    retries += 1

                # L·∫•y link t·ª´ c√°c item t√¨m ƒë∆∞·ª£c
                current_page_new_links = 0
                for item in items:
                    href = item.get('href')
                    if href:
                        if not href.startswith('http'): href = "https://vthmgroup.vn" + href

                        # --- LOGIC QUAN TR·ªåNG NH·∫§T ·ªû ƒê√ÇY ---
                        if href not in product_links:
                            product_links.add(href)
                            current_page_new_links += 1

                # In th√¥ng tin
                total_collected = len(product_links)
                msg = f"üìÑ Trang {page_count}: Th√™m {current_page_new_links} s·∫£n ph·∫©m m·ªõi. T·ªïng: {total_collected}"
                print(msg)
                if progress_callback: progress_callback(msg)

                # --- B∆Ø·ªöC 2: KI·ªÇM TRA ƒêI·ªÄU KI·ªÜN D·ª™NG (LOGIC T·ªîNG S·∫¢N PH·∫®M) ---
                # N·∫øu b·∫•m chuy·ªÉn trang r·ªìi m√† kh√¥ng l·∫•y th√™m ƒë∆∞·ª£c link n√†o m·ªõi -> ƒê√É H·∫æT
                if page_count == page_des:
                    break


                if current_page_new_links == 0 and page_count > 1:
                    print("üõë Kh√¥ng c√≥ s·∫£n ph·∫©m m·ªõi -> ƒê√£ ƒë·∫øn trang cu·ªëi.")
                    break

                # --- B∆Ø·ªöC 3: B·∫§M N√öT NEXT ---
                try:
                    next_btn = driver.find_element(By.CSS_SELECTOR, NEXT_BUTTON_SELECTOR)

                    # Cu·ªôn t·ªõi n√∫t v√† click
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_btn)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", next_btn)

                    print(f"‚è≥ ƒêang t·∫£i trang {page_count + 1}...")
                    page_count += 1

                    # Ch·ªù 1 ch√∫t sau khi click ƒë·ªÉ web b·∫Øt ƒë·∫ßu request
                    time.sleep(2)

                except Exception:
                    print(f"üõë Kh√¥ng t√¨m th·∫•y n√∫t Next (Ho·∫∑c n√∫t ƒë√£ b·ªã ·∫©n) -> D·ª´ng.")
                    break

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()

        return list(product_links)

    def parse_detail(self, soup, url):
        try:
            name_tag = soup.select_one('h1')
            product_name = name_tag.text.strip() if name_tag else "N/A"

            specs = {}
            # Grid items
            attr_items = soup.select('.attribute-item')
            for item in attr_items:
                lbl = item.select_one('.text-content-3')
                val = item.select_one('.text-content-1')
                if lbl and val: specs[lbl.text.strip().title()] = val.text.strip()

            # Flex items
            flex_rows = soup.select('.flex.gap-4')
            for row in flex_rows:
                lbl = row.select_one('.w-26.text-content-3')
                val = row.select_one('.text-content-1')
                if lbl and val: specs[lbl.text.strip().title()] = val.text.strip()

            images = []
            img_tags = soup.select('.slides img, .swiper-slide img, main img')
            for img in img_tags:
                src = img.get('src') or img.get('data-nuxt-img')
                if src and 'http' in src and not any(x in src.lower() for x in ['logo', 'icon', '.svg']):
                    q_index = src.find('?')
                    src = src[:q_index]
                    images.append(src)
            clean_images = list(set(images))

            final_data = {
                'URL': url, 'M√£ S·∫£n Ph·∫©m': product_name,
                'Th∆∞∆°ng Hi·ªáu': specs.get('Th∆∞∆°ng Hi·ªáu', 'N/A'), 'K√≠ch Th∆∞·ªõc': specs.get('K√≠ch Th∆∞·ªõc', 'N/A'),
                'B·ªÅ M·∫∑t': specs.get('B·ªÅ M·∫∑t', 'N/A'), 'X∆∞∆°ng G·∫°ch': specs.get('X∆∞∆°ng', 'N/A'),
                 'Danh S√°ch ·∫¢nh': clean_images
            }
            for k, v in specs.items():
                if k not in final_data: final_data[k] = v

            return final_data
        except Exception as e:
            print(f"L·ªói parse: {e}")
            return None


# --- CLASS 4: TaiceraVN (ƒê√£ t·ªëi ∆∞u l·∫•y chi ti·∫øt t·ª´ th·∫ª P) ---
# --- CLASS 4: TaiceraVN (B·∫£n n√¢ng c·∫•p: Smart Wait + Scroll) ---
class TaiceraScraper(BaseScraper):

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        driver = None
        product_links = set()

        # Selector n√∫t Next
        NEXT_BTN_XPATH = "//ul[contains(@class,'page-numbers')]//li/a[contains(@class,'next')]"

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()
            wait = WebDriverWait(driver, 15)  # Th·ªùi gian ch·ªù t·ªëi ƒëa 15s

            # --- B∆Ø·ªöC 1: QU√âT DANH M·ª§C ---
            target_urls = []
            is_general_page = "san-pham" in url or len(url.split('/')) < 5

            if is_general_page:
                if progress_callback: progress_callback(f"üîç ƒêang qu√©t menu t√¨m danh m·ª•c...")
                driver.get(url)
                time.sleep(3)
                soup = BeautifulSoup(driver.page_source, 'html.parser')

                menu_links = soup.select('#menu-item-1665 .sub-menu a')
                for a in menu_links:
                    href = a.get('href')
                    if href and 'http' in href: target_urls.append(href)

                if not target_urls:
                    see_more = soup.select('h3.section-title a')
                    for a in see_more:
                        href = a.get('href')
                        if href: target_urls.append(href)

                target_urls = list(set(target_urls))
                if progress_callback: progress_callback(f"‚úÖ T√¨m th·∫•y {len(target_urls)} danh m·ª•c. B·∫Øt ƒë·∫ßu c√†o.")
            else:
                target_urls.append(url)

            # --- B∆Ø·ªöC 2: C√ÄO CHI TI·∫æT ---
            total_cats = len(target_urls)
            for i, cat_url in enumerate(target_urls):
                msg = f"üìÇ [{i + 1}/{total_cats}] Danh m·ª•c: {cat_url}"
                print(msg)
                if progress_callback: progress_callback(msg)

                try:
                    driver.get(cat_url)
                    page_count = 1

                    while True:
                        # --- [M·ªöI] K·ª∏ THU·∫¨T CH·ªêNG S√ìT S·∫¢N PH·∫®M ---

                        # 1. Ch·ªù s·∫£n ph·∫©m xu·∫•t hi·ªán (Thay v√¨ sleep c·ª©ng)
                        try:
                            # Ch·ªù √≠t nh·∫•t 1 s·∫£n ph·∫©m xu·∫•t hi·ªán trong DOM
                            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, item_selector)))
                        except:
                            print("   ‚ö†Ô∏è Kh√¥ng th·∫•y s·∫£n ph·∫©m n√†o (C√≥ th·ªÉ trang tr·ªëng ho·∫∑c load l·ªói).")
                            break  # H·∫øt ho·∫∑c l·ªói

                        # 2. Cu·ªôn trang xu·ªëng cu·ªëi ƒë·ªÉ k√≠ch ho·∫°t Lazy Load (n·∫øu c√≥)
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)  # Ngh·ªâ 1 ch√∫t cho ·∫£nh/item load l√™n

                        # 3. L·∫•y d·ªØ li·ªáu
                        soup = BeautifulSoup(driver.page_source, 'html.parser')
                        items = soup.select(item_selector)

                        current_links_count = 0
                        for item in items:
                            tag = item.select_one(link_selector) if link_selector else item.select_one('a')
                            href = tag.get('href') if tag else None
                            if href:
                                if not href.startswith('http'): href = "https://taiceravn.com" + href
                                if href not in product_links:
                                    product_links.add(href)
                                    current_links_count += 1

                        print(f"   ‚Ü≥ Trang {page_count}: +{current_links_count} SP.")

                        # ƒêi·ªÅu ki·ªán d·ª´ng an to√†n
                        if current_links_count == 0 and page_count > 1:
                            # Th·ª≠ ƒë·ª£i th√™m 3s v√† qu√©t l·∫°i l·∫ßn cu·ªëi xem c√≥ ph·∫£i do m·∫°ng lag kh√¥ng
                            time.sleep(3)
                            soup = BeautifulSoup(driver.page_source, 'html.parser')
                            items = soup.select(item_selector)
                            if not items: break

                        # 4. Chuy·ªÉn trang
                        try:
                            next_btn = driver.find_element(By.XPATH, NEXT_BTN_XPATH)
                            next_href = next_btn.get_attribute('href')
                            if next_href:
                                driver.get(next_href)
                                page_count += 1
                            else:
                                break
                        except Exception:
                            break

                except Exception as e:
                    print(f"L·ªói danh m·ª•c {cat_url}: {e}")
                    continue

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()

        return list(product_links)

    def parse_detail(self, soup, url):
        # ... (Gi·ªØ nguy√™n h√†m parse_detail KH√îNG ƒê·ªîI) ...
        try:
            name_tag = soup.select_one('.product-title, h1.entry-title')
            product_name = name_tag.text.strip() if name_tag else "N/A"

            price_tag = soup.select_one('.price span.amount bdi')
            price_sale = soup.select_one('.price ins span.amount bdi')
            price = price_sale.text.strip() if price_sale else (price_tag.text.strip() if price_tag else "Li√™n h·ªá")

            images = []
            img_tags = soup.select('.product-gallery-slider img, .woocommerce-product-gallery__image img')
            for img in img_tags:
                src = img.get('src') or img.get('data-src') or img.get('data-large_image')
                if src and 'http' in src: images.append(src)
            images = list(set(images))

            specs = {}
            desc_content = soup.select_one('#tab-description, .woocommerce-Tabs-panel--description')
            if desc_content:
                paragraphs = desc_content.find_all('p')
                for p in paragraphs:
                    text = p.get_text().strip()
                    clean_text = text.lstrip('‚Äì- ').strip()
                    if ':' in clean_text:
                        parts = clean_text.split(':', 1)
                        specs[parts[0].strip().capitalize()] = parts[1].strip()
                    elif "ƒê∆°n gi√°" in clean_text:
                        specs["Th√¥ng tin gi√°"] = clean_text

            rows = soup.select('table.woocommerce-product-attributes tr')
            for row in rows:
                th = row.select_one('th')
                td = row.select_one('td')
                if th and td: specs[th.text.strip()] = td.text.strip()

            return {
                'URL': url,
                'T√™n S·∫£n Ph·∫©m': product_name,
                'Gi√°': price,
                '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A",
                'Danh S√°ch ·∫¢nh': images,
                **specs
            }
        except Exception as e:
            print(f"L·ªói parse Taicera: {e}")
            return None