import time
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# --- CLASS CHA (CH·ª®A C√ÅC H√ÄM CHUNG) ---
class BaseScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7'
        }

    def _setup_driver(self):
        """
        Kh·ªüi t·∫°o Selenium Driver.
        T·ª± ƒë·ªông x·ª≠ l√Ω c·∫£ m√¥i tr∆∞·ªùng Local (Windows/Mac) v√† Cloud (Linux).
        """
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Ch·∫°y ·∫©n. N·∫øu mu·ªën xem tr√¨nh duy·ªát ch·∫°y th√¨ comment d√≤ng n√†y l·∫°i.
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")

        # --- ∆ØU TI√äN 1: D√πng webdriver-manager (T·ªët nh·∫•t cho Local Windows) ---
        try:
            return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        except Exception as e:
            print(f"‚ö†Ô∏è Kh√¥ng d√πng ƒë∆∞·ª£c Webdriver Manager: {e}")

        # --- ∆ØU TI√äN 2: D√πng Driver m·∫∑c ƒë·ªãnh c·ªßa h·ªá th·ªëng (T·ªët cho Streamlit Cloud/Linux) ---
        try:
            return webdriver.Chrome(options=chrome_options)
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o Driver: {e}")
            return None  # Tr·∫£ v·ªÅ None n·∫øu th·∫•t b·∫°i to√†n t·∫≠p

    def get_links(self, url, item_selector, link_selector='a.link-load', progress_callback=None):
        """D√πng Selenium cu·ªôn trang v√† l·∫•y danh s√°ch Link."""

        driver = None  # Khai b√°o driver l√† None tr∆∞·ªõc ƒë·ªÉ tr√°nh l·ªói reference
        product_links = []

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")

            # Kh·ªüi t·∫°o driver
            driver = self._setup_driver()

            # N·∫øu driver kh·ªüi t·∫°o th·∫•t b·∫°i (v·∫´n l√† None) th√¨ n√©m l·ªói
            if not driver:
                raise Exception("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông tr√¨nh duy·ªát Chrome/Driver.")

            if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p: {url}")
            driver.get(url)
            time.sleep(2)

            if progress_callback: progress_callback("üîÑ ƒêang cu·ªôn trang (Lazy Load)...")
            last_height = driver.execute_script("return document.body.scrollHeight")

            while True:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            # Ph√¢n t√≠ch HTML ƒë·ªÉ l·∫•y link
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            items = soup.select(item_selector)

            if progress_callback: progress_callback(f"‚úÖ T√¨m th·∫•y {len(items)} th·∫ª s·∫£n ph·∫©m. ƒêang l·ªçc link...")

            domain = "/".join(url.split("/")[:3])

            for item in items[:6]:
                link_tag = item.select_one(link_selector)
                if link_tag and link_tag.get('href'):
                    full_link = link_tag.get('href')
                    if not full_link.startswith('http'):
                        full_link = domain + full_link
                    product_links.append(full_link)

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"L·ªói chi ti·∫øt: {e}")  # In ra terminal ƒë·ªÉ debug
        finally:
            # --- S·ª¨A L·ªñI T·∫†I ƒê√ÇY: Ch·ªâ quit() n·∫øu driver t·ªìn t·∫°i ---
            if driver:
                driver.quit()

        return list(set(product_links))

    def parse_detail(self, soup, url):
        """H√†m ·∫£o: Class con b·∫Øt bu·ªôc ph·∫£i vi·∫øt l·∫°i h√†m n√†y"""
        raise NotImplementedError

    def scrape_details_list(self, links, progress_bar=None, status_text=None):
        """D√πng Requests ƒë·ªÉ c√†o chi ti·∫øt danh s√°ch link"""
        data = []
        total = len(links)

        for i, link in enumerate(links):
            if status_text: status_text.text(f"ƒêang x·ª≠ l√Ω [{i + 1}/{total}]: {link}")
            if progress_bar: progress_bar.progress((i + 1) / total)

            try:
                response = requests.get(link, headers=self.headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    # G·ªçi h√†m ph√¢n t√≠ch c·ªßa Class con
                    detail = self.parse_detail(soup, link)
                    if detail:
                        data.append(detail)
            except Exception as e:
                print(f"L·ªói link {link}: {e}")

        return data


# --- CLASS CON 1: X·ª≠ l√Ω G·∫°ch & Ng√≥i ---
class ViglaceraTilesScraper(BaseScraper):
    def parse_detail(self, soup, url):
        # 1. Th√¥ng tin c∆° b·∫£n
        code_tag = soup.select_one('.title-main h2 strong')
        product_code = code_tag.text.strip() if code_tag else "N/A"

        breadcrumb = soup.select_one('.breadcrumb li:last-child a')
        collection = breadcrumb.text.strip() if breadcrumb else "N/A"

        # 2. H√¨nh ·∫£nh
        images = []
        for img in soup.select('.detail-pic img'):
            src = img.get('src')
            if src:
                if not src.startswith('http'):
                    src = "https://viglaceratiles.vn" + src
                images.append(src)

        # 3. Th√¥ng s·ªë k·ªπ thu·∫≠t
        dynamic_specs = {}
        for item in soup.select('.des-item'):
            key_tag = item.select_one('span')
            val_tag = item.select_one('h3')
            if key_tag and val_tag:
                key = key_tag.text.strip().title()
                value = val_tag.text.strip()
                dynamic_specs[key] = value

        final_data = {
            'URL': url,
            'M√£ S·∫£n Ph·∫©m': product_code,
            'B·ªô S∆∞u T·∫≠p': collection,
            '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A",
            'Danh S√°ch ·∫¢nh': images
        }
        final_data.update(dynamic_specs)
        return final_data


# --- CLASS CON 2: X·ª≠ l√Ω AAC ---
class ViglaceraAACScraper(BaseScraper):
    def parse_detail(self, soup, url):
        # 1. Th√¥ng tin c∆° b·∫£n
        name_tag = soup.find('h1', itemprop='name')
        product_name = name_tag.text.strip() if name_tag else "N/A"

        brand_tag = soup.select_one('.pro-brand a')
        brand = brand_tag.text.strip() if brand_tag else "N/A"

        type_tag = soup.select_one('.pro-type a')
        product_type = type_tag.text.strip() if type_tag else "N/A"

        # 2. H√¨nh ·∫£nh
        images = []
        main_img = soup.select_one('#ProductPhoto img')
        if main_img and main_img.get('src'):
            src = main_img.get('src')
            if src.startswith('//'): src = 'https:' + src
            images.append(src)

        for img in soup.select('#sliderproduct img'):
            src = img.get('src')
            if src:
                if src.startswith('//'): src = 'https:' + src
                if src not in images: images.append(src)

        # 3. B·∫£ng th√¥ng s·ªë
        specs = {}
        table = soup.find('table')
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all(['td', 'th'])
                row_data = [c.text.strip() for c in cols if c.text.strip()]

                if not row_data: continue
                if any(x in row_data[0].lower() for x in ['ch·ªâ ti√™u', 'th√¥ng s·ªë', 'ƒë∆°n v·ªã']):
                    continue

                if len(row_data) >= 2:
                    key = row_data[0]
                    value = row_data[-1]
                    if len(row_data) > 2:
                        key = f"{key} ({' '.join(row_data[1:-1])})"
                    specs[key] = value

        # 4. M√¥ t·∫£
        info_dict = {}
        headers = soup.find_all('h2')
        target_ul = None
        for h2 in headers:
            if any(x in h2.text.upper() for x in ["TH√îNG TIN", "T√çNH NƒÇNG"]):
                sibling = h2.find_next_sibling(['ul', 'div'])
                if sibling:
                    target_ul = sibling.find('ul') if sibling.name == 'div' else sibling
                    if target_ul: break

        if not target_ul:
            content_div = soup.find('div', class_='pro-tabcontent')
            if content_div: target_ul = content_div.find('ul')

        if target_ul:
            for i, li in enumerate(target_ul.find_all('li')):
                text = li.text.strip()
                if ':' in text:
                    k, v = text.split(':', 1)
                    info_dict[k.strip()] = v.strip()
                else:
                    info_dict[f"Th√¥ng tin {i + 1}"] = text

        final_data = {
            'URL': url,
            'T√™n S·∫£n Ph·∫©m': product_name,
            'Th∆∞∆°ng Hi·ªáu': brand,
            'Lo·∫°i S·∫£n Ph·∫©m': product_type,
            '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A",
            'Danh S√°ch ·∫¢nh': images
        }
        final_data.update(info_dict)
        final_data.update(specs)
        return final_data