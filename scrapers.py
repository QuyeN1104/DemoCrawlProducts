import time
import requests
import concurrent.futures
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager


# --- CLASS CHA (BASE) ---
class BaseScraper:
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        self.session.headers.update(self.headers)

    def _setup_driver(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        prefs = {"profile.managed_default_content_settings.images": 2}
        chrome_options.add_experimental_option("prefs", prefs)

        try:
            return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        except Exception:
            return webdriver.Chrome(options=chrome_options)

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        """M·∫∑c ƒë·ªãnh: D√πng Scroll (Cho Viglacera Tiles)"""
        driver = None
        product_links = []
        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()

            if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p: {url}")
            driver.get(url)
            time.sleep(3)

            if progress_callback: progress_callback("üîÑ ƒêang cu·ªôn trang (Lazy Load)...")
            last_height = driver.execute_script("return document.body.scrollHeight")

            while True:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            items = soup.select(item_selector)

            if progress_callback: progress_callback(f"‚úÖ T√¨m th·∫•y {len(items)} th·∫ª s·∫£n ph·∫©m. ƒêang tr√≠ch xu·∫•t link...")
            domain = "/".join(url.split("/")[:3])

            for item in items:
                tag = item.select_one(link_selector) if link_selector else item
                if tag and tag.get('href'):
                    href = tag.get('href')
                    if not href.startswith('http'): href = domain + href
                    product_links.append(href)
        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()
        return list(set(product_links))

    def parse_detail(self, soup, url):
        raise NotImplementedError

    def _fetch_single_product(self, link):
        try:
            response = self.session.get(link, timeout=15)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                return self.parse_detail(soup, link)
        except Exception as e:
            print(f"L·ªói link {link}: {e}")
        return None

    def scrape_details_list(self, links, progress_bar=None, status_text=None):
        data = []
        total = len(links)
        MAX_WORKERS = 10
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_url = {executor.submit(self._fetch_single_product, link): link for link in links}
            completed = 0
            for future in concurrent.futures.as_completed(future_to_url):
                result = future.result()
                if result: data.append(result)
                completed += 1
                if progress_bar: progress_bar.progress(completed / total)
                if status_text: status_text.text(f"ƒê√£ t·∫£i xong: {completed}/{total} s·∫£n ph·∫©m")
        return data


# --- CLASS 1: Viglacera Tiles ---
class ViglaceraTilesScraper(BaseScraper):
    def parse_detail(self, soup, url):
        code_tag = soup.select_one('.title-main h2 strong')
        product_code = code_tag.text.strip() if code_tag else "N/A"
        breadcrumb = soup.select_one('.breadcrumb li:last-child a')
        collection = breadcrumb.text.strip() if breadcrumb else "N/A"
        images = []
        for img in soup.select('.detail-pic img'):
            src = img.get('src')
            if src:
                if not src.startswith('http'): src = "https://viglaceratiles.vn" + src
                images.append(src)
        dynamic_specs = {}
        for item in soup.select('.des-item'):
            key_tag = item.select_one('span')
            val_tag = item.select_one('h3')
            if key_tag and val_tag: dynamic_specs[key_tag.text.strip().title()] = val_tag.text.strip()
        final_data = {
            'URL': url, 'M√£ S·∫£n Ph·∫©m': product_code, 'B·ªô S∆∞u T·∫≠p': collection,
            '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A", 'Danh S√°ch ·∫¢nh': images
        }
        final_data.update(dynamic_specs)
        return final_data


# --- CLASS 2: Viglacera AAC ---
class ViglaceraAACScraper(BaseScraper):
    def parse_detail(self, soup, url):
        name_tag = soup.find('h1', itemprop='name')
        product_name = name_tag.text.strip() if name_tag else "N/A"
        brand_tag = soup.select_one('.pro-brand a')
        brand = brand_tag.text.strip() if brand_tag else "N/A"
        type_tag = soup.select_one('.pro-type a')
        product_type = type_tag.text.strip() if type_tag else "N/A"
        images = []
        main_img = soup.select_one('#ProductPhoto img')
        if main_img and main_img.get('src'):
            src = main_img.get('src')
            if src.startswith('//'): src = 'https:' + src
            images.append(src)
        for img in soup.select('#sliderproduct img'):
            src = img.get('src')
            if src:
                if src.startswith('//'): src = 'https:' + src
                if src not in images: images.append(src)
        specs = {}
        table = soup.find('table')
        if table:
            for row in table.find_all('tr'):
                cols = row.find_all(['td', 'th'])
                row_data = [c.text.strip() for c in cols if c.text.strip()]
                if not row_data: continue
                if any(x in row_data[0].lower() for x in ['ch·ªâ ti√™u', 'th√¥ng s·ªë', 'ƒë∆°n v·ªã']): continue
                if len(row_data) >= 2:
                    key = row_data[0]
                    value = row_data[-1]
                    if len(row_data) > 2: key = f"{key} ({' '.join(row_data[1:-1])})"
                    specs[key] = value
        info_dict = {}
        headers = soup.find_all('h2')
        target_ul = None
        for h2 in headers:
            if any(x in h2.text.upper() for x in ["TH√îNG TIN", "T√çNH NƒÇNG"]):
                sibling = h2.find_next_sibling(['ul', 'div'])
                if sibling:
                    target_ul = sibling.find('ul') if sibling.name == 'div' else sibling
                    if target_ul: break
        if not target_ul:
            content_div = soup.find('div', class_='pro-tabcontent')
            if content_div: target_ul = content_div.find('ul')
        if target_ul:
            for i, li in enumerate(target_ul.find_all('li')):
                text = li.text.strip()
                if ':' in text:
                    k, v = text.split(':', 1)
                    info_dict[k.strip()] = v.strip()
                else:
                    info_dict[f"Th√¥ng tin {i + 1}"] = text
        final_data = {
            'URL': url, 'T√™n S·∫£n Ph·∫©m': product_name, 'Th∆∞∆°ng Hi·ªáu': brand,
            'Lo·∫°i S·∫£n Ph·∫©m': product_type, '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A", 'Danh S√°ch ·∫¢nh': images
        }
        final_data.update(info_dict)
        final_data.update(specs)
        return final_data


# --- CLASS 3: VTHM Group (Logic Data-Driven) ---
class VthmGroupScraper(BaseScraper):

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        driver = None
        product_links = set()  # S·ª≠ d·ª•ng SET ƒë·ªÉ t·ª± ƒë·ªông lo·∫°i b·ªè link tr√πng

        # Selector ch·ªâ d√πng ƒë·ªÉ T√åM n√∫t, kh√¥ng d√πng ƒë·ªÉ check disabled n·ªØa
        NEXT_BUTTON_SELECTOR = "nav.pagination button.btn-next"

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()

            if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p: {url}")
            driver.get(url)
            time.sleep(5)

            page_count = 1
            page_des = 20
            last_first_link = ""  # Bi·∫øn ƒë·ªÉ ki·ªÉm tra trang ƒë√£ load xong ch∆∞a

            while True:
                # --- B∆Ø·ªöC 1: L·∫§Y D·ªÆ LI·ªÜU ---
                # Ch·ªù t·ªëi ƒëa 10s cho ƒë·∫øn khi link s·∫£n ph·∫©m ƒë·∫ßu ti√™n thay ƒë·ªïi so v·ªõi trang tr∆∞·ªõc
                retries = 0
                items = []
                while retries < 10:
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    items = soup.select(item_selector)

                    if not items: break
                    current_first_link = items[0].get('href')

                    # N·∫øu l√† trang 1 HO·∫∂C link ƒë·∫ßu ti√™n ƒë√£ kh√°c trang tr∆∞·ªõc -> OK, trang m·ªõi ƒë√£ load
                    if page_count == 1 or current_first_link != last_first_link:
                        last_first_link = current_first_link
                        break

                    time.sleep(1)
                    retries += 1

                # L·∫•y link t·ª´ c√°c item t√¨m ƒë∆∞·ª£c
                current_page_new_links = 0
                for item in items:
                    href = item.get('href')
                    if href:
                        if not href.startswith('http'): href = "https://vthmgroup.vn" + href

                        # --- LOGIC QUAN TR·ªåNG NH·∫§T ·ªû ƒê√ÇY ---
                        if href not in product_links:
                            product_links.add(href)
                            current_page_new_links += 1

                # In th√¥ng tin
                total_collected = len(product_links)
                msg = f"üìÑ Trang {page_count}: Th√™m {current_page_new_links} s·∫£n ph·∫©m m·ªõi. T·ªïng: {total_collected}"
                print(msg)
                if progress_callback: progress_callback(msg)

                # --- B∆Ø·ªöC 2: KI·ªÇM TRA ƒêI·ªÄU KI·ªÜN D·ª™NG (LOGIC T·ªîNG S·∫¢N PH·∫®M) ---
                # N·∫øu b·∫•m chuy·ªÉn trang r·ªìi m√† kh√¥ng l·∫•y th√™m ƒë∆∞·ª£c link n√†o m·ªõi -> ƒê√É H·∫æT
                if page_count == page_des:
                    break


                if current_page_new_links == 0 and page_count > 1:
                    print("üõë Kh√¥ng c√≥ s·∫£n ph·∫©m m·ªõi -> ƒê√£ ƒë·∫øn trang cu·ªëi.")
                    break

                # --- B∆Ø·ªöC 3: B·∫§M N√öT NEXT ---
                try:
                    next_btn = driver.find_element(By.CSS_SELECTOR, NEXT_BUTTON_SELECTOR)

                    # Cu·ªôn t·ªõi n√∫t v√† click
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_btn)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", next_btn)

                    print(f"‚è≥ ƒêang t·∫£i trang {page_count + 1}...")
                    page_count += 1

                    # Ch·ªù 1 ch√∫t sau khi click ƒë·ªÉ web b·∫Øt ƒë·∫ßu request
                    time.sleep(2)

                except Exception:
                    print(f"üõë Kh√¥ng t√¨m th·∫•y n√∫t Next (Ho·∫∑c n√∫t ƒë√£ b·ªã ·∫©n) -> D·ª´ng.")
                    break

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()

        return list(product_links)

    def parse_detail(self, soup, url):
        try:
            name_tag = soup.select_one('h1')
            product_name = name_tag.text.strip() if name_tag else "N/A"

            specs = {}
            # Grid items
            attr_items = soup.select('.attribute-item')
            for item in attr_items:
                lbl = item.select_one('.text-content-3')
                val = item.select_one('.text-content-1')
                if lbl and val: specs[lbl.text.strip().title()] = val.text.strip()

            # Flex items
            flex_rows = soup.select('.flex.gap-4')
            for row in flex_rows:
                lbl = row.select_one('.w-26.text-content-3')
                val = row.select_one('.text-content-1')
                if lbl and val: specs[lbl.text.strip().title()] = val.text.strip()

            images = []
            img_tags = soup.select('.slides img, .swiper-slide img, main img')
            for img in img_tags:
                src = img.get('src') or img.get('data-nuxt-img')
                if src and 'http' in src and not any(x in src.lower() for x in ['logo', 'icon', '.svg']):
                    q_index = src.find('?')
                    src = src[:q_index]
                    images.append(src)
            clean_images = list(set(images))

            final_data = {
                'URL': url, 'M√£ S·∫£n Ph·∫©m': product_name,
                'Th∆∞∆°ng Hi·ªáu': specs.get('Th∆∞∆°ng Hi·ªáu', 'N/A'), 'K√≠ch Th∆∞·ªõc': specs.get('K√≠ch Th∆∞·ªõc', 'N/A'),
                'B·ªÅ M·∫∑t': specs.get('B·ªÅ M·∫∑t', 'N/A'), 'X∆∞∆°ng G·∫°ch': specs.get('X∆∞∆°ng', 'N/A'),
                 'Danh S√°ch ·∫¢nh': clean_images
            }
            for k, v in specs.items():
                if k not in final_data: final_data[k] = v

            return final_data
        except Exception as e:
            print(f"L·ªói parse: {e}")
            return None


# --- CLASS 4: TaiceraVN (ƒê√£ t·ªëi ∆∞u l·∫•y chi ti·∫øt t·ª´ th·∫ª P) ---
# --- CLASS 4: TaiceraVN (B·∫£n n√¢ng c·∫•p: Smart Wait + Scroll) ---
# --- CLASS 4: TaiceraVN (ƒê√£ th√™m logic c√†o Slider 80x80) ---
class TaiceraScraper(BaseScraper):

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        driver = None
        product_links = set()

        # Selector n√∫t Next c·ªßa trang ph√¢n trang (Archive Page)
        NEXT_BTN_XPATH_ARCHIVE = "//ul[contains(@class,'page-numbers')]//li/a[contains(@class,'next')]"

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()
            wait = WebDriverWait(driver, 15)

            # --- GIAI ƒêO·∫†N 1: T√åM LINK DANH M·ª§C (LINK CON) ---
            target_urls = []
            is_general_page = "san-pham" in url or len(url.split('/')) < 5

            if is_general_page:
                if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p trang ch·ªß s·∫£n ph·∫©m ƒë·ªÉ qu√©t...")
                driver.get(url)
                time.sleep(3)

                # === [LOGIC M·ªöI] C√ÄO TR·ª∞C TI·∫æP T·ª™ SLIDER TRANG CH·ª¶ (ƒê·∫∂C BI·ªÜT L√Ä 80x80) ===
                try:
                    print("‚ö° ƒêang k√≠ch ho·∫°t ch·∫ø ƒë·ªô c√†o Slider (G·∫°ch 80x80)...")
                    # 1. T√¨m ti√™u ƒë·ªÅ "G·∫°ch 80 x 80 cm"
                    # XPath n√†y t√¨m th·∫ª h3 ch·ª©a text, sau ƒë√≥ l·∫•y cha l√† .col-inner ƒë·ªÉ khoanh v√πng
                    slider_section_xpath = "//h3[contains(., '80 x 80') or contains(., '80x80')]/ancestor::div[contains(@class, 'col-inner')]"

                    # Ki·ªÉm tra xem c√≥ t√¨m th·∫•y v√πng 80x80 kh√¥ng
                    slider_containers = driver.find_elements(By.XPATH, slider_section_xpath)

                    if slider_containers:
                        container = slider_containers[0]  # L·∫•y v√πng ƒë·∫ßu ti√™n t√¨m th·∫•y
                        if progress_callback: progress_callback(
                            f"‚ö° Ph√°t hi·ªán Slider 80x80. ƒêang l·∫•y d·ªØ li·ªáu tr·ª±c ti·∫øp...")

                        # Th·ª≠ click Next kho·∫£ng 10 l·∫ßn ƒë·ªÉ load h·∫øt ·∫£nh trong slider
                        # V√¨ slider n√†y l·∫∑p l·∫°i (wrapAround: true), ta c·∫ßn set ƒë·ªÉ l·ªçc tr√πng
                        for _ in range(10):
                            # L·∫•y link hi·ªán t·∫°i trong v√πng n√†y
                            soup_slider = BeautifulSoup(container.get_attribute('outerHTML'), 'html.parser')
                            links_in_slider = soup_slider.select("div.product-small a.woocommerce-LoopProduct-link")

                            count_new = 0
                            for a in links_in_slider:
                                href = a.get('href')
                                if href:
                                    if not href.startswith('http'): href = "https://taiceravn.com" + href
                                    if href not in product_links:
                                        product_links.add(href)
                                        count_new += 1

                            print(f"   -> Slider 80x80: L·∫•y {count_new} link m·ªõi.")

                            # T√¨m n√∫t Next TRONG V√ôNG N√ÄY (quan tr·ªçng)
                            try:
                                # D√πng d·∫•u ch·∫•m .// ƒë·ªÉ ch·ªâ t√¨m con c·ªßa container
                                next_btn_slider = container.find_element(By.XPATH,
                                                                         ".//button[contains(@class, 'next')]")
                                driver.execute_script("arguments[0].click();", next_btn_slider)
                                time.sleep(1.5)  # Ch·ªù slider tr∆∞·ª£t
                            except Exception as e:
                                print("   -> Kh√¥ng b·∫•m ƒë∆∞·ª£c n√∫t Next slider (ho·∫∑c h·∫øt):", e)
                                break
                    else:
                        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y m·ª•c G·∫°ch 80x80 tr√™n trang ch·ªß.")

                except Exception as e:
                    print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω Slider: {e}")

                # === [H·∫æT LOGIC M·ªöI] TI·∫æP T·ª§C QU√âT DANH M·ª§C KH√ÅC ===

                soup = BeautifulSoup(driver.page_source, 'html.parser')

                # T√¨m trong Menu ch√≠nh
                menu_links = soup.select('#menu-item-1665 .sub-menu a')
                for a in menu_links:
                    href = a.get('href')
                    if href and 'http' in href: target_urls.append(href)

                # T√¨m c√°c n√∫t "XEM TH√äM"
                see_more_links = soup.select('h3.section-title a')
                for a in see_more_links:
                    href = a.get('href')
                    if href and 'http' in href: target_urls.append(href)

                target_urls = list(set(target_urls))

                # Lo·∫°i b·ªè link 80x80 kh·ªèi danh s√°ch qu√©t chi ti·∫øt (v√¨ trang ƒë√≥ b·ªã l·ªói nh∆∞ b·∫°n n√≥i)
                # Ho·∫∑c c·ª© ƒë·ªÉ n√≥ ch·∫°y, n·∫øu l·ªói th√¨ try/except b√™n d∆∞·ªõi s·∫Ω b·ªè qua
                if progress_callback: progress_callback(
                    f"‚úÖ ƒê√£ qu√©t xong trang ch·ªß. T√¨m th·∫•y {len(product_links)} sp t·ª´ slider v√† {len(target_urls)} danh m·ª•c.")
            else:
                target_urls.append(url)

            # --- GIAI ƒêO·∫†N 2: DUY·ªÜT C√ÅC DANH M·ª§C C√íN L·∫†I ---
            total_cats = len(target_urls)
            for i, cat_url in enumerate(target_urls):
                # N·∫øu b·∫°n mu·ªën b·ªè qua trang 80x80 b·ªã l·ªói ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian
                if "80x80" in cat_url and len(product_links) > 0:
                    print(f"‚è© B·ªè qua danh m·ª•c 80x80 (ƒë√£ c√†o t·ª´ slider): {cat_url}")
                    continue

                msg = f"üìÇ [{i + 1}/{total_cats}] ƒêang x·ª≠ l√Ω: {cat_url}"
                print(msg)
                if progress_callback: progress_callback(msg)

                try:
                    driver.get(cat_url)
                    time.sleep(3)

                    page_count = 1

                    while True:
                        # Ch·ªù v√† scroll (Ch·ªëng s√≥t)
                        try:
                            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, item_selector)))
                        except:
                            break

                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)

                        soup = BeautifulSoup(driver.page_source, 'html.parser')
                        items = soup.select(item_selector)

                        current_links_count = 0
                        for item in items:
                            tag = item.select_one(link_selector) if link_selector else item.select_one('a')
                            href = tag.get('href') if tag else None
                            if href:
                                if not href.startswith('http'): href = "https://taiceravn.com" + href
                                if href not in product_links:
                                    product_links.add(href)
                                    current_links_count += 1

                        if current_links_count == 0 and page_count > 1:
                            break

                            # Chuy·ªÉn trang (Archive)
                        try:
                            next_btn = driver.find_element(By.XPATH, NEXT_BTN_XPATH_ARCHIVE)
                            next_href = next_btn.get_attribute('href')
                            if next_href:
                                driver.get(next_href)
                                page_count += 1
                            else:
                                break
                        except Exception:
                            break

                except Exception as e:
                    print(f"L·ªói danh m·ª•c {cat_url}: {e}")
                    continue

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()

        return list(product_links)

    def parse_detail(self, soup, url):
        # ... (Gi·ªØ nguy√™n h√†m parse_detail) ...
        try:
            name_tag = soup.select_one('.product-title, h1.entry-title')
            product_name = name_tag.text.strip() if name_tag else "N/A"

            price_tag = soup.select_one('.price span.amount bdi')
            price_sale = soup.select_one('.price ins span.amount bdi')
            price = price_sale.text.strip() if price_sale else (price_tag.text.strip() if price_tag else "Li√™n h·ªá")

            images = []
            img_tags = soup.select('.product-gallery-slider img, .woocommerce-product-gallery__image img')
            for img in img_tags:
                src = img.get('src') or img.get('data-src') or img.get('data-large_image')
                if src and 'http' in src: images.append(src)
            images = list(set(images))

            specs = {}
            desc_content = soup.select_one('#tab-description, .woocommerce-Tabs-panel--description')
            if desc_content:
                paragraphs = desc_content.find_all('p')
                for p in paragraphs:
                    text = p.get_text().strip()
                    clean_text = text.lstrip('‚Äì- ').strip()
                    if ':' in clean_text:
                        parts = clean_text.split(':', 1)
                        specs[parts[0].strip().capitalize()] = parts[1].strip()
                    elif "ƒê∆°n gi√°" in clean_text:
                        specs["Th√¥ng tin gi√°"] = clean_text

            rows = soup.select('table.woocommerce-product-attributes tr')
            for row in rows:
                th = row.select_one('th')
                td = row.select_one('td')
                if th and td: specs[th.text.strip()] = td.text.strip()

            return {
                'URL': url,
                'T√™n S·∫£n Ph·∫©m': product_name,
                'Gi√°': price,
                '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A",
                'Danh S√°ch ·∫¢nh': images,
                **specs
            }
        except Exception as e:
            print(f"L·ªói parse Taicera: {e}")
            return None


# --- CLASS 5: Slabstone (X·ª≠ l√Ω AJAX Pagination & ƒêa Tab chi ti·∫øt) ---
class SlabstoneScraper(BaseScraper):

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        driver = None
        product_links = set()

        # Selector n√∫t Next
        NEXT_BTN_SELECTOR = "a.tv-page.next"

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()

            if progress_callback: progress_callback(f"üîó ƒêang truy c·∫≠p: {url}")
            driver.get(url)
            time.sleep(3)

            page_count = 1

            while True:
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                items = soup.select(item_selector)

                current_page_links = []
                for item in items:
                    tag = item.select_one(link_selector) if link_selector else item.select_one('a')
                    href = tag.get('href') if tag else None
                    if href:
                        if not href.startswith('http'): href = "https://slabstone.vn" + href
                        if href not in product_links:
                            product_links.add(href)
                            current_page_links.append(href)

                msg = f"üìÑ Trang {page_count}: T√¨m th·∫•y {len(current_page_links)} s·∫£n ph·∫©m m·ªõi. (T·ªïng: {len(product_links)})"
                print(msg)
                if progress_callback: progress_callback(msg)

                if len(current_page_links) == 0 and page_count > 1:
                    print("üõë Kh√¥ng c√≥ s·∫£n ph·∫©m m·ªõi -> ƒê√£ ƒë·∫øn trang cu·ªëi.")
                    break

                try:
                    next_btn = driver.find_element(By.CSS_SELECTOR, NEXT_BTN_SELECTOR)
                    if not next_btn.is_displayed():
                        print("üö´ N√∫t Next b·ªã ·∫©n -> H·∫øt trang.")
                        break

                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", next_btn)
                    time.sleep(1)
                    driver.execute_script("arguments[0].click();", next_btn)

                    print(f"‚è≥ ƒêang t·∫£i trang {page_count + 1}...")
                    time.sleep(3)  # Ch·ªù AJAX load
                    page_count += 1
                except Exception:
                    print(f"üõë Kh√¥ng t√¨m th·∫•y n√∫t Next (Ho·∫∑c ƒë√£ h·∫øt trang).")
                    break

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()

        return list(product_links)

    # --- H√ÄM PH·ª§ ƒê·ªÇ L·∫§Y TH√îNG S·ªê T·ª™ 1 PANEL ---
    def _parse_specs_from_panel(self, container):
        specs = {}
        # T√¨m c√°c d√≤ng th√¥ng s·ªë trong class .tv-info-grid .item
        items = container.select('.item')
        for item in items:
            lbl = item.select_one('label')
            val_p = item.select_one('p')
            val_img = item.select_one('.item-img img')  # Tr∆∞·ªùng h·ª£p C√¥ng ngh·ªá x∆∞∆°ng l√† ·∫£nh

            if lbl:
                key = lbl.text.strip().replace(':', '')
                val = "N/A"
                if val_p:
                    val = val_p.text.strip()
                elif val_img:
                    # N·∫øu gi√° tr·ªã l√† ·∫£nh (v√≠ d·ª• icon VeinTech), l·∫•y link ·∫£nh
                    val = val_img.get('src')

                if key and val:
                    specs[key] = val
        return specs

    def parse_detail(self, soup, url):
        try:
            # 1. T√™n chung s·∫£n ph·∫©m
            name_tag = soup.select_one('h1.elementor-heading-title')
            product_name = name_tag.text.strip() if name_tag else "N/A"

            # 2. M√¥ t·∫£ chung
            desc_tag = soup.select_one('.elementor-widget-theme-post-content')
            description = desc_tag.text.strip() if desc_tag else ""

            # 3. ·∫¢nh (L·∫•y t·ª´ Slider, l·ªçc tr√πng)
            images = []
            img_tags = soup.select('.swiper-slide:not(.swiper-slide-duplicate) img')
            for img in img_tags:
                src = img.get('src')
                if src: images.append(src)
            images = list(set(images))

            # 4. X·ª¨ L√ù ƒêA BI·∫æN TH·ªÇ (TABs) [QUAN TR·ªåNG]
            variants = []

            # T√¨m danh s√°ch c√°c Tab (M√£ s·∫£n ph·∫©m: SP82H127, SM82H127...)
            tab_navs = soup.select('.tv-tab-nav li')

            if tab_navs:
                # N·∫øu c√≥ nhi·ªÅu Tab
                for li in tab_navs:
                    variant_code = li.text.strip()  # L·∫•y t√™n m√£ (VD: SP82H127)
                    panel_id = li.get('data-tab')  # L·∫•y ID c·ªßa panel ch·ª©a d·ªØ li·ªáu (VD: tv-tab-0)

                    # T√¨m panel t∆∞∆°ng ·ª©ng trong HTML
                    panel = soup.select_one(f'#{panel_id}')
                    if panel:
                        # G·ªçi h√†m ph·ª• ƒë·ªÉ l·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t c·ªßa panel n√†y
                        specs = self._parse_specs_from_panel(panel)

                        # Th√™m v√†o danh s√°ch bi·∫øn th·ªÉ
                        variants.append({
                            "M√£": variant_code,
                            **specs  # G·ªôp c√°c th√¥ng s·ªë (K√≠ch th∆∞·ªõc, ƒê·ªô d√†y...)
                        })
            else:
                # Tr∆∞·ªùng h·ª£p kh√¥ng c√≥ Tab (ch·ªâ c√≥ 1 lo·∫°i duy nh·∫•t)
                # Th·ª≠ t√¨m b·∫£ng th√¥ng s·ªë tr·ª±c ti·∫øp
                panel = soup.select_one('.tv-info-grid')
                if panel:
                    specs = self._parse_specs_from_panel(panel)
                    variants.append({
                        "M√£": "Ti√™u chu·∫©n",
                        **specs
                    })

            # 5. Tr·∫£ v·ªÅ d·ªØ li·ªáu
            return {
                'URL': url,
                'T√™n S·∫£n Ph·∫©m': product_name,
                'M√¥ t·∫£': description,
                '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A",
                'Danh S√°ch ·∫¢nh': images,
                'Chi Ti·∫øt C√°c M√£': variants  # Tr·∫£ v·ªÅ danh s√°ch c√°c bi·∫øn th·ªÉ
            }
        except Exception as e:
            print(f"L·ªói parse Slabstone: {e}")
            return None


# --- CLASS 6: Amy.vn (Full: Qu√©t Menu + Cu·ªôn trang + Parse chi ti·∫øt chu·∫©n) ---
class AmyScraper(BaseScraper):

    def get_links(self, url, item_selector, link_selector=None, progress_callback=None):
        driver = None
        product_links = set()

        try:
            if progress_callback: progress_callback(f"üöÄ ƒêang kh·ªüi ƒë·ªông tr√¨nh duy·ªát...")
            driver = self._setup_driver()
            # Amy.vn load animation kh√° l√¢u, ch·ªù t·ªëi ƒëa 20s
            wait = WebDriverWait(driver, 20)

            # --- GIAI ƒêO·∫†N 1: T·ª∞ ƒê·ªòNG L·∫§Y LINK DANH M·ª§C T·ª™ MENU ---
            target_categories = []

            # Ki·ªÉm tra n·∫øu l√† trang ch·ªß
            is_homepage = "amy.vn" in url and len(url.split('/')) < 4

            if is_homepage:
                if progress_callback: progress_callback(f"‚è≥ ƒêang truy c·∫≠p trang ch·ªß v√† ch·ªù Menu...")
                driver.get(url)

                try:
                    # Ch·ªù menu xu·∫•t hi·ªán
                    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".sub-menu-drop")))

                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    # L·∫•y t·∫•t c·∫£ link trong menu con
                    menu_items = soup.select('.sub-menu-drop .item-menu-second a')

                    for item in menu_items:
                        href = item.get('href')
                        name = item.text.strip()
                        if href:
                            if not href.startswith('http'): href = "https://amy.vn" + href
                            target_categories.append(href)
                            print(f"   -> T√¨m th·∫•y danh m·ª•c: {name}")

                except Exception as e:
                    print(f"‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c menu (L·ªói: {e}). S·∫Ω th·ª≠ c√†o URL hi·ªán t·∫°i.")
                    target_categories.append(url)
            else:
                target_categories.append(url)

            target_categories = list(set(target_categories))
            if progress_callback: progress_callback(
                f"‚úÖ ƒê√£ t√¨m th·∫•y {len(target_categories)} danh m·ª•c. B·∫Øt ƒë·∫ßu qu√©t s·∫£n ph·∫©m.")

            # --- GIAI ƒêO·∫†N 2: DUY·ªÜT T·ª™NG DANH M·ª§C & CU·ªòN V√î T·∫¨N ---
            total_cats = len(target_categories)
            for i, cat_url in enumerate(target_categories):
                msg = f"üìÇ [{i + 1}/{total_cats}] ƒêang x·ª≠ l√Ω: {cat_url}"
                print(msg)
                if progress_callback: progress_callback(msg)

                try:
                    driver.get(cat_url)
                    time.sleep(5)  # Ch·ªù load trang danh m·ª•c

                    # Logic cu·ªôn trang (Infinite Scroll)
                    last_height = driver.execute_script("return document.body.scrollHeight")
                    scroll_retries = 0

                    while True:
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(3)  # Ch·ªù s·∫£n ph·∫©m m·ªõi load l√™n

                        new_height = driver.execute_script("return document.body.scrollHeight")
                        if new_height == last_height:
                            scroll_retries += 1
                            if scroll_retries >= 2: break  # H·∫øt trang
                        else:
                            scroll_retries = 0
                            last_height = new_height

                        # (T√πy ch·ªçn) In ra s·ªë l∆∞·ª£ng t·∫°m th·ªùi
                        # items_now = len(driver.find_elements(By.CSS_SELECTOR, item_selector))
                        # print(f"   ...ƒê√£ load {items_now} s·∫£n ph·∫©m")

                    # Sau khi cu·ªôn xong, parse HTML 1 l·∫ßn ƒë·ªÉ l·∫•y link
                    soup_cat = BeautifulSoup(driver.page_source, 'html.parser')
                    items = soup_cat.select(item_selector)

                    count_new = 0
                    for item in items:
                        # Link n·∫±m trong th·∫ª a c√≥ class .link-load ho·∫∑c .more-details
                        tag = item.select_one(link_selector) if link_selector else item.select_one('a')
                        href = tag.get('href') if tag else None

                        if href:
                            if not href.startswith('http'): href = "https://amy.vn" + href
                            if href not in product_links:
                                product_links.add(href)
                                count_new += 1

                    print(f"   -> L·∫•y ƒë∆∞·ª£c {count_new} s·∫£n ph·∫©m m·ªõi.")

                except Exception as e:
                    print(f"L·ªói danh m·ª•c {cat_url}: {e}")
                    continue

        except Exception as e:
            if progress_callback: progress_callback(f"‚ùå L·ªói Selenium: {e}")
            print(f"Error: {e}")
        finally:
            if driver: driver.quit()

        return list(product_links)

    def parse_detail(self, soup, url):
        try:
            # 1. T√™n s·∫£n ph·∫©m (Th·∫ª h1)
            name_tag = soup.select_one('h1')
            product_name = name_tag.text.strip() if name_tag else "N/A"

            # 2. H√¨nh ·∫£nh
            # ·∫¢nh n·∫±m trong .details-pics -> .slidebox-item -> img
            images = []
            img_tags = soup.select('.details-pics .slidebox-item img')
            for img in img_tags:
                src = img.get('src') or img.get('data-src')
                if src:
                    if not src.startswith('http'): src = "https://amy.vn" + src
                    images.append(src)
            images = list(set(images))  # L·ªçc tr√πng

            # 3. Th√¥ng s·ªë k·ªπ thu·∫≠t (D·ª±a tr√™n HTML b·∫°n g·ª≠i)
            specs = {}

            # T√¨m div ch·ª©a th√¥ng tin
            info_container = soup.select_one('.product-info.data-index')

            if info_container:
                # T√¨m c√°c th·∫ª h3 class="des-item"
                items = info_container.select('.des-item')
                for item in items:
                    # Key n·∫±m trong span, Value n·∫±m trong strong
                    key_tag = item.select_one('span')
                    val_tag = item.select_one('strong')

                    if key_tag and val_tag:
                        # X√≥a d·∫•u : ·ªü key (VD: "M√£:" -> "M√£")
                        key = key_tag.text.replace(':', '').strip()
                        value = val_tag.text.strip()
                        specs[key] = value

            return {
                'URL': url,
                'T√™n S·∫£n Ph·∫©m': product_name,
                '·∫¢nh ƒê·∫°i Di·ªán': images[0] if images else "N/A",
                'Danh S√°ch ·∫¢nh': images,
                **specs  # G·ªôp M√£, Gi√°, Th∆∞∆°ng hi·ªáu, K√≠ch th∆∞·ªõc, B·ªÅ m·∫∑t, X∆∞∆°ng...
            }
        except Exception as e:
            print(f"L·ªói parse Amy: {e}")
            return None